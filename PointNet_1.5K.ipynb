{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = r\"E:\\20_PIGS\\S-C-D-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbe7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(DATA_DIR):\n",
    "    data_points = []\n",
    "    data_labels = []\n",
    "    num_points = 1500\n",
    "    i = 0\n",
    "    for names in os.listdir(DATA_DIR):\n",
    "        i = i+1\n",
    "        DATA_DIR_DATE = os.path.join(DATA_DIR, names)\n",
    "        print(DATA_DIR_DATE)\n",
    "        for name in os.listdir(DATA_DIR_DATE):\n",
    "            DIR_TO_PLY = os.path.join(DATA_DIR_DATE, name)\n",
    "            for name in os.listdir(DIR_TO_PLY):\n",
    "                final_dir = os.path.join(DIR_TO_PLY, name)\n",
    "                pcd1 = o3d.io.read_point_cloud(final_dir)\n",
    "                pcd = pcd1.voxel_down_sample(voxel_size=15)\n",
    "                n_points = np.asarray(pcd.points).shape[0]\n",
    "                idx = np.random.choice(n_points, num_points, replace=False)\n",
    "                pcd_downsampled = pcd.select_by_index(idx)\n",
    "                # Compute centroid of point cloud\n",
    "#                 centroid = pcd_downsampled.get_center()\n",
    "#                 pcd_downsampled.translate(-centroid)\n",
    "#                 distances = pcd_downsampled.get_max_bound()\n",
    "#                 pcm = np.asarray(pcd_downsampled.points)\n",
    "#                 pcm[:, 0] /= distances[0]\n",
    "#                 pcm[:, 1] /= distances[1]\n",
    "#                 pcm[:, 2] /= distances[2]\n",
    "        #         print(final_dir)\n",
    "                data_points.append(pcd_downsampled.points)\n",
    "                data_labels.append(i-1)\n",
    "    #         np.array(data_points)\n",
    "    #         np.asarray(data_labels)\n",
    "#         break\n",
    "    train_points, test_points, train_labels, test_labels = train_test_split(data_points, data_labels, test_size=0.2, random_state=42)\n",
    "#     print(np.array(train_points))\n",
    "#     print(np.array(train_labels))\n",
    "    return (\n",
    "      np.array(train_points),\n",
    "      np.array(train_labels),\n",
    "      np.array(test_points),\n",
    "      np.array(test_labels)\n",
    "\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33883dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points, train_labels, test_points, test_labels = parse_dataset(DIR)\n",
    "# print(train_points.shape)\n",
    "# print(train_labels.shape)\n",
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.01, 0.01, dtype=tf.float64)\n",
    "    # shuffle points\n",
    "    points = tf.random.shuffle(points)\n",
    "    return points, label\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(32) #map(augment).batch(32)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(32)\n",
    "\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60784497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnet(inputs, num_features):\n",
    "\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1500, 3))\n",
    "\n",
    "x = tnet(inputs, 3)\n",
    "x = conv_bn(x, 32)\n",
    "x = conv_bn(x, 32)\n",
    "x = tnet(x, 32)\n",
    "x = conv_bn(x, 32)\n",
    "x = conv_bn(x, 64)\n",
    "x = conv_bn(x, 512)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = dense_bn(x, 256)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = dense_bn(x, 128)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(18, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "# model.load_weights(r\"C:\\Users\\spaudel6\\Desktop\\PointNet_3k\\Day1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fe72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=200, validation_data=test_dataset, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d31adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(r\"C:\\Users\\spaudel6\\Desktop\\PointNet_Scripts\\DAY1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
